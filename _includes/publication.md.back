
## <i class="fa fa-chevron-right"></i> Conference

<font size=3><span style='background-color: #ffffd0'>The reviewed conference papers are shown as follows:</span></font>

<h5>First Author, Corresponding Author</h5>
<table class="table table-hover">
  
<tr id="tr-song22_interspeech" >
  <td>
    <li>
      <font size=2>
      <u>Hao&nbsp;Shi</u>, Yuchun&nbsp;Shu, Longbiao&nbsp;Wang, Jianwu&nbsp;Dang, and Tatsuya&nbsp;Kawahara.<br>
      Fusing Multiple Bandwidth Spectrograms for Improving Speech Enhancement.&nbsp;<em><a href='publications/apsipa-2022-shi-resolutions.pdf' target='_blank'  style='text-decoration: none;'>[ref]</a> </em><br>
      In Proc. APSIPA ASC, pp.1935--1940, 2022.<br> 
      </font>
    </li>
  </td>
</tr>  
  
<tr id="tr-song22_interspeech" >
  <td>
    <li>
      <font size=2>
      <u>Hao&nbsp;Shi</u>, Longbiao&nbsp;Wang, Sheng&nbsp;Li, Jianwu&nbsp;Dang, and Tatsuya&nbsp;Kawahara.<br>
      Subband-Based Spectrogram Fusion for Speech Enhancement by Combining Mapping and Masking Approaches.&nbsp;<em><a href='publications/apsipa-2022-shi-subband.pdf' target='_blank'>[ref]</a> </em><br>
      In Proc. APSIPA ASC, pp.286--292, 2022.<br> 
      </font>
    </li>
  </td>
</tr>  
  
<tr id="tr-shi22_interspeech">
  <td>
    <li>
      <font size=2>
      <u>Hao&nbsp;Shi</u>, Longbiao&nbsp;Wang, Sheng&nbsp;Li, Jianwu&nbsp;Dang, and Tatsuya&nbsp;Kawahara.<br>
      Monaural speech enhancement based on spectrogram decomposition for convolutional neural network-sensitive feature extraction.&nbsp;<em><a href='publications/interspeech-2022-shi.pdf' target='_blank'>[ref]</a> </em><br>
      In Proc. INTERSPEECH, pp.221--225, 2022.<br>
      </font>
    </li>
  </td>
</tr>
    
<tr id="tr-song22_interspeech" >
  <td>
    <li>
      <font size=2>
      Tongtong&nbsp;Song, Qiang&nbsp;Xu, Meng&nbsp;Ge, Longbiao&nbsp;Wang, <u>Hao&nbsp;Shi</u>, Yongjie&nbsp;Lv, Yuqin&nbsp;Lin, and Jianwu&nbsp;Dang.<br>
      Language-specific Characteristic Assistance for Code-switching Speech Recognition.&nbsp;<em><a href='publications/interspeech-2022-song.pdf' target='_blank'>[ref]</a> </em><br>
      In Proc. INTERSPEECH, pp.3924--3928, 2022.<br> 
      <font color=Blue>(Corresponding author)</font>
      </font>
    </li>
  </td>
</tr>
    
<tr id="tr-xu22_interspeech" >
  <td>
    <li>
      <font size=2>
      Qiang&nbsp;Xu, Tongtong&nbsp;Song, Longbiao&nbsp;Wang, <u>Hao&nbsp;Shi</u>, Yuqin&nbsp;Lin, Yongjie&nbsp;Lv, Meng&nbsp;Ge, Qiang&nbsp;Yu, and Jianwu&nbsp;Dang.<br>
      Self-Distillation Based on High-level Information Supervision for Compressing End-to-End ASR Model.&nbsp;<em><a href='publications/interspeech-2022-xu.pdf' target='_blank'>[ref]</a> </em><br>
      In Proc. INTERSPEECH, pp.1716--1720, 2022.<br> 
      <font color=Blue>(Corresponding author)</font>
      </font>
    </li>
  </td>
</tr>    
    
<tr id="tr-shi21_apsipa">
  <td>
    <li>
      <font size=2>
      <u>Hao&nbsp;Shi</u>, Longbiao&nbsp;Wang, Sheng&nbsp;Li, Cunhang&nbsp;Fan, Jianwu&nbsp;Dang, and Tatsuya&nbsp;Kawahara.<br>
      Spectrograms Fusion-based End-to-end Robust Automatic Speech Recognition.&nbsp;<em><a href='publications/apsipa-2021-shi.pdf' target='_blank'>[ref]</a> </em><br>
      In Proc. APSIPA ASC, pp.438--442, 2021.<br>
      </font>
    </li>
  </td>
</tr>

<tr id="tr-qiang21_iconip">
  <td>
    <li>
      <font size=2>
      Luya&nbsp;Qiang, <u>Hao&nbsp;Shi</u>, Meng&nbsp;Ge, Haoran&nbsp;Yin, Nan&nbsp;Li, Longbiao&nbsp;Wang, Sheng&nbsp;Li, and Jianwu&nbsp;Dang.<br>
      Speech Dereverberation Based on Scale-aware Mean Square Error Loss.&nbsp;<em><a href='publications/iconip-2021-qiang.pdf' target='_blank'>[ref]</a> </em><br>
      In Proc of ICONIP, pp.55--63, 2021.<br> 
      <font color=Blue>(Joint first author, equal contribution)</font>
      </font>
    </li>
  </td>
</tr>

<tr id="tr-yin21_iconip">
  <td>
    <li>
      <font size=2>
      Haoran&nbsp;Yin, <u>Hao&nbsp;Shi</u>, Longbiao&nbsp;Wang, Luya&nbsp;Qiang, Sheng&nbsp;Li, Meng&nbsp;Ge, Gaoyan&nbsp;Zhang, and Jianwu&nbsp;Dang.<br>
      Simultaneous Progressive Filtering-based Monaural Speech Enhancement.&nbsp;<em><a href='publications/iconip-2021-yin.pdf' target='_blank'>[ref]</a> </em><br>
      In Proc. ICONIP, pp.213--221, 2021.<br>
      <font color=Blue>(Joint first author, equal contribution)</font>
      </font>
    </li>
  </td>
</tr>
    
<tr id="tr-shi20_interspeech">
  <td>
    <li>
      <font size=2>
      <u>Hao&nbsp;Shi</u>, Longbiao&nbsp;Wang, Sheng&nbsp;Li, Chenchen&nbsp;Ding, Meng&nbsp;Ge, Nan&nbsp;Li, Jianwu&nbsp;Dang, and Hiroshi&nbsp;Seki.<br>
      Singing Voice Extraction with Attention-Based Spectrograms Fusion.&nbsp;<em><a href='publications/interspeech-2020-shi.pdf' target='_blank'>[ref]</a> </em><br>
      In Proc. INTERSPEECH, pp.2412--2416, 2020.<br>
      </font>
    </li>
  </td>
</tr>

<tr id="tr-9054661">
  <td>
    <li>
      <font size=2>
      <u>Hao&nbsp;Shi</u>, Longbiao&nbsp;Wang, Meng&nbsp;Ge, Sheng&nbsp;Li, and Jianwu&nbsp;Dang.<br>
      Spectrograms Fusion with Minimum Difference Masks Estimation for Monaural Speech Dereverberation.&nbsp;<em><a href='publications/icassp-2020-shi.pdf' target='_blank'>[ref]</a> </em><br>
      In Proc. ICASSP, pp.7539--7543, 2020.<br>
      </font>
    </li>
  </td>
</tr>
</table>



<h5>Co-author</h5>
<table class="table table-hover">
  
<tr id="tr-yang2022_iscslp" >
  <td>
    <li>
      <font size=2>
      Yanbing&nbsp;Yang, <u>Hao&nbsp;Shi</u>, Yuqin&nbsp;Lin, Meng&nbsp;Ge, Longbiao&nbsp;Wang, Qingzhi&nbsp;Hou and Jianwu&nbsp;Dang.<br>
      Adaptive Attention Network with Domain Adversarial Training for Multi-Accent Speech Recognition.&nbsp;<em><a href='publications/iscslp-2022-yang.pdf' target='_blank'>[ref]</a> </em><br>
      In Proc. ISCSLP, pp.--, 2022.<br>
      </font>
    </li>
  </td>
</tr>  
  
<tr id="tr-ge19_interspeech" >
  <td>
    <li>
      <font size=2>
      Meng&nbsp;Ge, Longbiao&nbsp;Wang, Nan&nbsp;Li, <u>Hao&nbsp;Shi</u>, Jianwu&nbsp;Dang, and Xiangang&nbsp;Li.<br>
      Environment-Dependent Attention-Driven Recurrent Convolutional Neural Network for Robust Speech Enhancement.&nbsp;<em><a href='publications/interspeech-2019-ge.pdf' target='_blank'>[ref]</a> </em><br>
      In Proc. INTERSPEECH, pp.3151--3157, 2019.<br>
      </font>
    </li>
  </td>
</tr>

</table>

