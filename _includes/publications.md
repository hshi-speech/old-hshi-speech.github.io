###### <i class="fa fa-chevron-right"></i> Jounral (Reviewed)
<table class="table table-hover">
<tr id="tr-shi2024_taslp" >
  <td style="padding-left: 20px;">
    <li>
      <font size=2>
      Waveform-domain Speech Enhancement Using Spectrogram Encoding for Robust Speech Recognition.&nbsp;
      <em><a href='publications/taslp-2024-shi.pdf' target='_blank'  style='text-decoration: none;'>[PDF]</a> </em>
      <em><a href='bib/taslp-2024-shi.txt' target='_blank'>[.bib]</a> </em><br>
      <u>Hao&nbsp;Shi</u>, Masato&nbsp;Mimura, and Tatsuya&nbsp;Kawahara.<br>
      IEEE/ACM Transactions on Audio, Speech, and Language Processing, pp.xx--xx, 2024.<br> 
      </font>
    </li>
  </td>
</tr> 
</table>



###### <i class="fa fa-chevron-right"></i> Conference (Reviewed)

<table class="table table-hover">
<tr id="tr-shi2024_hscma" >
  <td style="padding-left: 20px;">
    <li>
      <font size=2>
      Ensemble Inference for Diffusion Model-based Speech Enhancement.&nbsp;
      <em><a href='publications/hscma-2024-shi.pdf' target='_blank'  style='text-decoration: none;'>[PDF]</a> </em>
      <em><a href='bib/hscma-2024-shi.txt' target='_blank'>[.bib]</a> </em><br>
      <u>Hao&nbsp;Shi</u>, Naoyuki&nbsp;Kamo, Marc&nbsp;Delcroix, Tomohiro&nbsp;Nakatani, and Shoko&nbsp;Araki.<br>
      In Proc. HSCMA, pp.xx--xx, 2024.<br> 
      </font>
    </li>
  </td>
</tr> 
<tr id="tr-shi2024_icassp" >
  <td style="padding-left: 20px;">
    <li>
      <font size=2>
      Diffusion-Based Speech Enhancement with Joint Generative and Predictive Decoders.&nbsp;
      <em><a href='publications/icassp-2024-shi.pdf' target='_blank'  style='text-decoration: none;'>[PDF]</a> </em>
      <em><a href='bib/icassp-2024-shi.txt' target='_blank'>[.bib]</a> </em><br>
      <u>Hao&nbsp;Shi</u>, Kazuki&nbsp;Shimada, Masato&nbsp;Hirano, Takashi&nbsp;Shibuya, Yuichiro&nbsp;Koyama, Zhi&nbsp;Zhong, Shusuke&nbsp;Takahashi, Tatsuya&nbsp;Kawahara, and Yuki&nbsp;Mitsufuji.<br>
      In Proc. ICASSP, pp.12951--12955, 2024.<br> 
      </font>
    </li>
  </td>
</tr>  
<tr id="tr-gao2024_icassp" >
  <td style="padding-left: 20px;">
    <li>
      <font size=2>
      Enhancing Two-stage Finetuning for Speech Emotion Recognition Using Adapters.&nbsp;
      <em><a href='publications/icassp-2024-gao.pdf' target='_blank'>[PDF]</a> </em>
      <em><a href='bib/icassp-2024-gao.txt' target='_blank'>[.bib]</a> </em><br>
      Yuan&nbsp;Gao, <u>Hao&nbsp;Shi</u>,  Chenhui&nbsp;Chu, Tatsuya&nbsp;Kawahara<br>
      In Proc. ICASSP, pp.11316--11320, 2024.<br>
      </font>
    </li>
  </td>
</tr> 


<tr id="tr-zhong2023_waspaa" >
  <td style="padding-left: 20px;">
    <li>
      <font size=2>
      Extending Audio Masked Autoencoders Toward Audio Restoration.&nbsp;
      <em><a href='https://arxiv.org/pdf/2305.06701.pdf' target='_blank'>[PDF]</a> </em>
      <em><a href='bib/waspaa-2023-zhong.txt' target='_blank'>[.bib]</a> </em><br>
      Zhi&nbsp;Zhong, <u>Hao&nbsp;Shi</u>, Masato&nbsp;Hirano, Kazuki&nbsp;Shimada, Kazuya&nbsp;Tateishi, Takashi&nbsp;Shibuya, Shusuke&nbsp;Takahashi, Yuki&nbsp;Mitsufuji.<br>
      In Proc. WASPAA, pp.1--5, 2023.<br>
      </font>
    </li>
  </td>
</tr> 
<tr id="tr-shi_icassp2023" >
  <td style="padding-left: 20px;">
    <li>
      <font size=2>
      Time-domain Speech Enhancement Assisted by Multi-resolution Frequency Encoder and Decoder.&nbsp;
      <em><a href='publications/icassp-2023-shi-tf.pdf' target='_blank'  style='text-decoration: none;'>[PDF]</a> </em>
      <em><a href='bib/icassp-2023-shi-tf.txt' target='_blank'  style='text-decoration: none;'>[.bib]</a> </em><br>
      <u>Hao&nbsp;Shi</u>, Masato&nbsp;Mimura, Longbiao&nbsp;Wang, Jianwu&nbsp;Dang, and Tatsuya&nbsp;Kawahara.<br>
      In Proc. ICASSP, pp.1--5, 2023.<br> 
      </font>
    </li>
  </td>
</tr>  

  
<tr id="tr-shi_resolutions_apsipa" >
  <td style="padding-left: 20px;">
    <li>
      <font size=2>
      Fusing Multiple Bandwidth Spectrograms for Improving Speech Enhancement.&nbsp;
      <em><a href='publications/apsipa-2022-shi-resolutions.pdf' target='_blank'  style='text-decoration: none;'>[PDF]</a> </em>
      <em><a href='bib/apsipa-2022-shi-resolutions.txt' target='_blank'  style='text-decoration: none;'>[.bib]</a> </em><br>
      <u>Hao&nbsp;Shi</u>, Yuchun&nbsp;Shu, Longbiao&nbsp;Wang, Jianwu&nbsp;Dang, and Tatsuya&nbsp;Kawahara.<br>
      In Proc. APSIPA ASC, pp.1935--1940, 2022.<br> 
      </font>
    </li>
  </td>
</tr>  
  
<tr id="tr-shi_subband_apsipa" >
  <td style="padding-left: 20px;">
    <li>
      <font size=2>
      Subband-Based Spectrogram Fusion for Speech Enhancement by Combining Mapping and Masking Approaches.&nbsp;
      <em><a href='publications/apsipa-2022-shi-subband.pdf' target='_blank'>[PDF]</a> </em>
      <em><a href='bib/apsipa-2022-shi-subband.txt' target='_blank'>[.bib]</a> </em><br>
      <u>Hao&nbsp;Shi</u>, Longbiao&nbsp;Wang, Sheng&nbsp;Li, Jianwu&nbsp;Dang, and Tatsuya&nbsp;Kawahara.<br>
      In Proc. APSIPA ASC, pp.286--292, 2022.<br> 
      </font>
    </li>
  </td>
</tr>  
  
<tr id="tr-shi22_interspeech">
  <td style="padding-left: 20px;">
    <li>
      <font size=2>
      Monaural speech enhancement based on spectrogram decomposition for convolutional neural network-sensitive feature extraction.&nbsp;
      <em><a href='publications/interspeech-2022-shi.pdf' target='_blank'>[PDF]</a> </em>
      <em><a href='bib/interspeech-2022-shi.txt' target='_blank'>[.bib]</a> </em><br>
      <u>Hao&nbsp;Shi</u>, Longbiao&nbsp;Wang, Sheng&nbsp;Li, Jianwu&nbsp;Dang, and Tatsuya&nbsp;Kawahara.<br>
      In Proc. INTERSPEECH, pp.221--225, 2022.<br>
      </font>
    </li>
  </td>
</tr>
    
<tr id="tr-song22_interspeech" >
  <td style="padding-left: 20px;">
    <li>
      <font size=2>
      Language-specific Characteristic Assistance for Code-switching Speech Recognition.&nbsp;
      <em><a href='publications/interspeech-2022-song.pdf' target='_blank'>[PDF]</a> </em>
      <em><a href='bib/interspeech-2022-song.txt' target='_blank'>[.bib]</a> </em><br>
      Tongtong&nbsp;Song, Qiang&nbsp;Xu, Meng&nbsp;Ge, Longbiao&nbsp;Wang, <u>Hao&nbsp;Shi</u>, Yongjie&nbsp;Lv, Yuqin&nbsp;Lin, and Jianwu&nbsp;Dang.<br>
      In Proc. INTERSPEECH, pp.3924--3928, 2022.<br> 
      <font color=Blue>(Corresponding author)</font>
      </font>
    </li>
  </td>
</tr>
    
<tr id="tr-xu22_interspeech" >
  <td style="padding-left: 20px;">
    <li>
      <font size=2>
      Self-Distillation Based on High-level Information Supervision for Compressing End-to-End ASR Model.&nbsp;
      <em><a href='publications/interspeech-2022-xu.pdf' target='_blank'>[PDF]</a> </em>
      <em><a href='bib/interspeech-2022-xu.txt' target='_blank'>[.bib]</a> </em><br>
      Qiang&nbsp;Xu, Tongtong&nbsp;Song, Longbiao&nbsp;Wang, <u>Hao&nbsp;Shi</u>, Yuqin&nbsp;Lin, Yongjie&nbsp;Lv, Meng&nbsp;Ge, Qiang&nbsp;Yu, and Jianwu&nbsp;Dang.<br>
      In Proc. INTERSPEECH, pp.1716--1720, 2022.<br> 
      <font color=Blue>(Corresponding author)</font>
      </font>
    </li>
  </td>
</tr>    
<tr id="tr-yang2022_iscslp" >
  <td style="padding-left: 20px;">
    <li>
      <font size=2>
      Adaptive Attention Network with Domain Adversarial Training for Multi-Accent Speech Recognition.&nbsp;
      <em><a href='publications/iscslp-2022-yang.pdf' target='_blank'>[PDF]</a> </em>
      <em><a href='bib/iscslp-2022-yang.txt' target='_blank'>[.bib]</a> </em><br>
      Yanbing&nbsp;Yang, <u>Hao&nbsp;Shi</u>, Yuqin&nbsp;Lin, Meng&nbsp;Ge, Longbiao&nbsp;Wang, Qingzhi&nbsp;Hou and Jianwu&nbsp;Dang.<br>
      In Proc. ISCSLP, pp.6--10, 2022.<br>
      </font>
    </li>
  </td>
</tr>  


<tr id="tr-shi21_apsipa">
  <td style="padding-left: 20px;">
    <li>
      <font size=2>
      Spectrograms Fusion-based End-to-end Robust Automatic Speech Recognition.&nbsp;
      <em><a href='publications/apsipa-2021-shi.pdf' target='_blank'>[PDF]</a></em>
      <em><a href='bib/apsipa-2021-shi.txt' target='_blank'>[.bib]</a></em><br>
      <u>Hao&nbsp;Shi</u>, Longbiao&nbsp;Wang, Sheng&nbsp;Li, Cunhang&nbsp;Fan, Jianwu&nbsp;Dang, and Tatsuya&nbsp;Kawahara.<br>
      In Proc. APSIPA ASC, pp.438--442, 2021.<br>
      </font>
    </li>
  </td>
</tr>

<tr id="tr-qiang21_iconip">
  <td style="padding-left: 20px;">
    <li>
      <font size=2>
      Speech Dereverberation Based on Scale-aware Mean Square Error Loss.&nbsp;
      <em><a href='publications/iconip-2021-qiang.pdf' target='_blank'>[PDF]</a> </em>
      <em><a href='bib/iconip-2021-qiang.txt' target='_blank'>[.bib]</a> </em><br>
      Luya&nbsp;Qiang, <u>Hao&nbsp;Shi</u>, Meng&nbsp;Ge, Haoran&nbsp;Yin, Nan&nbsp;Li, Longbiao&nbsp;Wang, Sheng&nbsp;Li, and Jianwu&nbsp;Dang.<br>
      In Proc of ICONIP, pp.55--63, 2021.<br> 
      <font color=Blue>(Joint first author, equal contribution)</font>
      </font>
    </li>
  </td>
</tr>

<tr id="tr-yin21_iconip">
  <td style="padding-left: 20px;">
    <li>
      <font size=2>
      Simultaneous Progressive Filtering-based Monaural Speech Enhancement.&nbsp;
      <em><a href='publications/iconip-2021-yin.pdf' target='_blank'>[PDF]</a></em>
      <em><a href='bib/iconip-2021-yin.txt' target='_blank'>[.bib]</a></em>
      <br>
      Haoran&nbsp;Yin, <u>Hao&nbsp;Shi</u>, Longbiao&nbsp;Wang, Luya&nbsp;Qiang, Sheng&nbsp;Li, Meng&nbsp;Ge, Gaoyan&nbsp;Zhang, and Jianwu&nbsp;Dang.<br>
      In Proc. ICONIP, pp.213--221, 2021.<br>
      <font color=Blue>(Joint first author, equal contribution)</font>
      </font>
    </li>
  </td>
</tr>


<tr id="tr-shi20_interspeech">
  <td style="padding-left: 20px;">
    <li>
      <font size=2>
      Singing Voice Extraction with Attention-Based Spectrograms Fusion.&nbsp;
      <em><a href='publications/interspeech-2020-shi.pdf' target='_blank'>[PDF]</a> </em>
      <em><a href='bib/interspeech-2020-shi.txt' target='_blank'>[.bib]</a> </em><br>
      <u>Hao&nbsp;Shi</u>, Longbiao&nbsp;Wang, Sheng&nbsp;Li, Chenchen&nbsp;Ding, Meng&nbsp;Ge, Nan&nbsp;Li, Jianwu&nbsp;Dang, and Hiroshi&nbsp;Seki.<br>
      In Proc. INTERSPEECH, pp.2412--2416, 2020.<br>
      </font>
    </li>
  </td>
</tr>

<tr id="tr-9054661">
  <td style="padding-left: 20px;">
    <li>
      <font size=2>
      Spectrograms Fusion with Minimum Difference Masks Estimation for Monaural Speech Dereverberation.&nbsp;
      <em><a href='publications/icassp-2020-shi.pdf' target='_blank'>[PDF]</a> </em>
      <em><a href='bib/icassp-2020-shi.txt' target='_blank'>[.bib]</a> </em>
      <br>
      <u>Hao&nbsp;Shi</u>, Longbiao&nbsp;Wang, Meng&nbsp;Ge, Sheng&nbsp;Li, and Jianwu&nbsp;Dang.<br>
      In Proc. ICASSP, pp.7539--7543, 2020.<br>
      </font>
    </li>
  </td>
</tr>


<tr id="tr-ge19_interspeech" >
  <td style="padding-left: 20px;">
    <li>
      <font size=2>
      Environment-Dependent Attention-Driven Recurrent Convolutional Neural Network for Robust Speech Enhancement.&nbsp;
      <em><a href='publications/interspeech-2019-ge.pdf' target='_blank'>[PDF]</a> </em>
      <em><a href='bib/interspeech-2019-ge.txt' target='_blank'>[.bib]</a> </em><br>
      Meng&nbsp;Ge, Longbiao&nbsp;Wang, Nan&nbsp;Li, <u>Hao&nbsp;Shi</u>, Jianwu&nbsp;Dang, and Xiangang&nbsp;Li.<br>
      In Proc. INTERSPEECH, pp.3151--3157, 2019.<br>
      </font>
    </li>
  </td>
</tr>

</table>


###### <i class="fa fa-chevron-right"></i> Reports
<table class="table table-hover">

<tr id="tr-shi2023_report" >
  <td style="padding-left: 20px;">
    <li>
      <font size=2>
      Investigation of Adapter for Automatic Speech Recognition in Noisy Environment.&nbsp;
      <em><a href='https://arxiv.org/pdf/2402.18275.pdf' target='_blank'>[PDF]</a> </em>
      <em><a href='bib/report-2023-shi.txt' target='_blank'>[.bib]</a> </em><br>
      <u>Hao&nbsp;Shi</u>, Tatsuya&nbsp;Kawahara.<br>
      In SIG Technical Reports, pp.1--6, 2023.<br>
      </font>
    </li>
  </td>
</tr>
</table>
