
## <i class="fa fa-chevron-right"></i> Publications

Representative publications that I am a primary author on are
<span style='background-color: #ffffd0'>highlighted.</span><br>
[<a href="https://scholar.google.com/citations?user=DclFbLwAAAAJ&hl">Google Scholar</a>]


<h3>First Author</h3>
<table class="table table-hover">
<tr id="tr-shi22_interspeech">
<td>
<font size=2>
    Hao&nbsp;Shi, Longbiao&nbsp;Wang, Sheng&nbsp;Li, Jianwu&nbsp;Dang, and Tatsuya&nbsp;Kawahara.<br>
    Monaural speech enhancement based on spectrogram decomposition for convolutional neural network-sensitive feature extraction.&nbsp;<em><a href='' target='_blank'>[ref]</a> </em><br>
    In Proc. Interspeech, pp.438--442, 2022.<br>
</font>
</td>
</tr>
    
<tr id="tr-shi21_apsipa">
<td>
<font size=2>
    Hao&nbsp;Shi, Longbiao&nbsp;Wang, Sheng&nbsp;Li, Cunhang&nbsp;Fan, Jianwu&nbsp;Dang, and Tatsuya&nbsp;Kawahara.<br>
    Spectrograms Fusion-based End-to-end Robust Automatic Speech Recognition.&nbsp;<em><a href='https://github.com/hshi-speech/resume/blob/main/pdf/APSIPA-2021.pdf' target='_blank'>[ref]</a> </em><br>
    In Proc. APSIPA ASC, pp.438--442, 2021.<br>
</font>
</td>
</tr>

<tr id="tr-shi20_interspeech">
<td>
<font size=2>
    Hao&nbsp;Shi, Longbiao&nbsp;Wang, Sheng&nbsp;Li, Chenchen&nbsp;Ding, Meng&nbsp;Ge, Nan&nbsp;Li, Jianwu&nbsp;Dang, and Hiroshi&nbsp;Seki.<br>
    Singing Voice Extraction with Attention-Based Spectrograms Fusion.&nbsp;<em><a href='https://github.com/hshi-speech/resume/blob/main/pdf/Wed-1-11-1.pdf' target='_blank'>[ref]</a> </em><br>
    In Proc. INTERSPEECH, pp.2412--2416, 2020.<br>
</font>
</td>
</tr>

<tr id="tr-9054661">
<td>
<font size=2>
    Hao&nbsp;Shi, Longbiao&nbsp;Wang, Meng&nbsp;Ge, Sheng&nbsp;Li, and Jianwu&nbsp;Dang.<br>
    Spectrograms Fusion with Minimum Difference Masks Estimation for Monaural Speech Dereverberation.&nbsp;<em><a href='https://github.com/hshi-speech/resume/blob/main/pdf/0007539.pdf' target='_blank'>[ref]</a> </em><br>
    In Proc. ICASSP, pp.7539--7543, 2020.<br>
</font>
</td>
</tr>
</table>



<h3>Co-first Author</h3>
<table class="table table-hover">

<tr id="tr-qiang21_iconip">
<td>
<font size=2>
    Luya&nbsp;Qiang, Hao&nbsp;Shi, Meng&nbsp;Ge, Haoran&nbsp;Yin, Nan&nbsp;Li, Longbiao&nbsp;Wang, Sheng&nbsp;Li, and Jianwu&nbsp;Dang.<br>
    Speech Dereverberation Based on Scale-aware Mean Square Error Loss.&nbsp;<em><a href='https://github.com/hshi-speech/resume/blob/main/pdf/SaSD.pdf' target='_blank'>[ref]</a> </em><br>
    In Proc of ICONIP, pp.55--63, 2021.<br>
</font>
</td>
</tr>


<tr id="tr-yin21_iconip">
<td>
<font size=2>
    Haoran&nbsp;Yin, Hao&nbsp;Shi, Longbiao&nbsp;Wang, Luya&nbsp;Qiang, Sheng&nbsp;Li, Meng&nbsp;Ge, Gaoyan&nbsp;Zhang, and Jianwu&nbsp;Dang.<br>
    Simultaneous Progressive Filtering-based Monaural Speech Enhancement.&nbsp;<em><a href='https://github.com/hshi-speech/resume/blob/main/pdf/iconip2021-yin.pdf' target='_blank'>[ref]</a> </em><br>
    In Proc. ICONIP, pp.213--221, 2021.<br>
</font>
</td>
</tr>
</table>



<h3>Co-author</h3>
<table class="table table-hover">
<tr id="tr-ge19_interspeech" >
<td>
<font size=2>
    Meng&nbsp;Ge, Longbiao&nbsp;Wang, Nan&nbsp;Li, Hao&nbsp;Shi, Jianwu&nbsp;Dang, and Xiangang&nbsp;Li.<br>
    Environment-Dependent Attention-Driven Recurrent Convolutional Neural Network for Robust Speech Enhancement.&nbsp;<em><a href='https://github.com/hshi-speech/resume/blob/main/pdf/1477.pdf' target='_blank'>[ref]</a> </em><br>
    In Proc. INTERSPEECH, pp.3151--3157, 2019.<br>
</font>
</td>
</tr>

</table>

